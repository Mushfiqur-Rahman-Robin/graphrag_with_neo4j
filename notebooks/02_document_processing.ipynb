{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f3c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # add project root, not src\n",
    "\n",
    "from src.document_loader import DocumentProcessor\n",
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805eef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents found. Creating sample document...\n",
      "‚úÖ Sample document created: /home/mushfiq/Desktop/graphrag_with_neo4j/notebooks/../documents/sample_graphrag.docx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_sample_document():\n",
    "    \"\"\"Create a sample document for testing\"\"\"\n",
    "    sample_content = \"\"\"\n",
    "    GraphRAG System Implementation\n",
    "\n",
    "    This document describes the implementation of a GraphRAG system using LangChain, Gemini, and Neo4j.\n",
    "\n",
    "    Key Components:\n",
    "    1. Document Processing - Uses semantic chunking to process documents\n",
    "    2. Knowledge Graph Creation - Extracts entities and relationships using LLM\n",
    "    3. Vector Storage - Stores embeddings for similarity search\n",
    "    4. Hybrid Retrieval - Combines graph traversal with vector search\n",
    "\n",
    "    Technologies Used:\n",
    "    - LangChain: Framework for building LLM applications\n",
    "    - Gemini: Google's large language model for text processing\n",
    "    - Neo4j: Graph database for storing relationships\n",
    "    - Python: Programming language for implementation\n",
    "\n",
    "    Benefits:\n",
    "    - Better context understanding through relationships\n",
    "    - More accurate retrieval through graph traversal  \n",
    "    - Explainable results through graph structure\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_file = config.DOCUMENTS_DIR / \"sample_graphrag.docx\"\n",
    "    \n",
    "    try:\n",
    "        from docx import Document as DocxDocument\n",
    "        doc = DocxDocument()\n",
    "        doc.add_paragraph(sample_content)\n",
    "        doc.save(sample_file)\n",
    "        print(f\"‚úÖ Sample document created: {sample_file}\")\n",
    "    except ImportError:\n",
    "        # Fallback - create a text file\n",
    "        with open(config.DOCUMENTS_DIR / \"sample_graphrag.txt\", \"w\") as f:\n",
    "            f.write(sample_content)\n",
    "        print(\"‚úÖ Sample text file created (install python-docx for .docx support)\")\n",
    "\n",
    "# Create sample if no documents exist\n",
    "if not list(config.DOCUMENTS_DIR.glob(\"*.docx\")):\n",
    "    print(\"No documents found. Creating sample document...\")\n",
    "    create_sample_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a86ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 documents to process...\n",
      "Processing: sample_graphrag.docx\n",
      "  Created 2 chunks\n",
      "Total: 2 document chunks loaded\n",
      "\n",
      "üìä Document Statistics:\n",
      "Total documents: 2\n",
      "Average chunk length: 434\n",
      "\n",
      "üìù Sample chunks:\n",
      "Chunk 1 (from sample_graphrag.docx):\n",
      "  Content: GraphRAG System Implementation\n",
      "\n",
      "    This document describes the implementation of a GraphRAG system using LangChain, Gemini, and Neo4j. Key Components:\n",
      "    1....\n",
      "  Metadata: {'source': 'sample_graphrag.docx', 'chunk_id': 0, 'doc_id': 'sample_graphrag'}\n",
      "\n",
      "Chunk 2 (from sample_graphrag.docx):\n",
      "  Content: Document Processing - Uses semantic chunking to process documents\n",
      "    2. Knowledge Graph Creation - Extracts entities and relationships using LLM\n",
      "    3. Vector Storage - Stores embeddings for similari...\n",
      "  Metadata: {'source': 'sample_graphrag.docx', 'chunk_id': 1, 'doc_id': 'sample_graphrag'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processor = DocumentProcessor()\n",
    "documents = processor.load_documents()\n",
    "\n",
    "if documents:\n",
    "    print(\"\\nüìä Document Statistics:\")\n",
    "    print(f\"Total documents: {len(documents)}\")\n",
    "    print(f\"Average chunk length: {sum(len(doc.page_content) for doc in documents) // len(documents)}\")\n",
    "    \n",
    "    # Show sample chunks\n",
    "    print(\"\\nüìù Sample chunks:\")\n",
    "    for i, doc in enumerate(documents[:3]):\n",
    "        print(f\"Chunk {i+1} (from {doc.metadata['source']}):\")\n",
    "        print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No documents loaded. Please add .docx files to the documents folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e926c179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Documents saved for later use\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if documents:\n",
    "    with open('../processed_documents.pkl', 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    print(\"‚úÖ Documents saved for later use\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag_with_neo4j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
